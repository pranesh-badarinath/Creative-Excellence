import numpy as np
import matplotlib.pyplot as pit
import pandas as pd
from sklearn.datasets import load_boston

%matplotlib inline

boston= load_boston()

print(boston.DESCR)

features=pd.DataFrame(boston.data,columns=boston.feature_names)
target=pd.DataFrame(boston.target,columns=['Target'])
data=pd.concat([features,target],axis=1)

data2=data.corr('pearson')
data2

abs(data2.loc['Target']).sort_values(ascending=False)

x=data['LSTAT']
y=data['Target']

x=np.array(x/x.mean())
y=np.array(y/y.mean())

n=int(0.8*len(x))
x_train=x[:n]
y_train=y[:n]

x_test=x[n:]
y_test=y[n:]

pit.plot(x_train,y_train,'g.')

def hypothisis(a,b,x):
  return a*x+b
  
def error(a,b,x,y):
   e=0
   m=len(y)
   for i in range(m):
    e +=np.power((hypothisis(a,b,x[i])-y[i]),2)

   return(1/2*m)*e
      
def step_gradient(a,b,x,y,learning_rate):
  grad_a=0
  grad_b=0
  m=len(x)
  for i in range(m):
    grad_a+=1/m*(hypothisis(a,b,x[i])-y[i])*x[i]
    grad_b+=1/m*(hypothisis(a,b,x[i])-y[i])
    a=a-(grad_a*learning_rate)
    b=b-(grad_b*learning_rate)

  return a,b

def descend(initial_a,initial_b,x,y,learning_rate,itterations):
  a=initial_b
  b=initial_b
  for i in range(itterations):
    e=error(a,b,x,y)
    if i % 1000==0:
      print(f"Error: {e},a:{a},b:{b}")

    a,b=step_gradient(a,b,x,y,learning_rate)
  return a,b

a=0
b=5
learning_rate=0.0001
itterations=10000
final_a,final_b=descend(a,b,x_train,y_train,learning_rate,itterations)

print(error(a,b,x_train,y_train))

print(error(final_a,final_b,x_test,y_test))

pit.plot(x_test,y_test,'b.',x_test,hypothisis(final_a,final_b,x_test),'g')

